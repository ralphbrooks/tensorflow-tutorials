{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing a Data Pipeline in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to go from a blank slate (an empty Jupyter Notebook in Amazon Sagemaker) all the way to state of the art results in Natural Language Processing using deep learning. This particular notebook uses TensorFlow in order to construct a data pipeline. The other notebook in this repository (\"2-Sentiment-Classification-with-BERT.ipynb\") builds on the work that we have done here; it uses the data pipeline to classify Yelp reviews as positive or negativate. \n",
    "\n",
    "Notes: \n",
    "\n",
    "1) The notebook is constructed with teaching in mind, and as a result, all of the primary code is displayed within this notebook.\n",
    "\n",
    "2) In order to teach effectively, this notebook emphasizes clarity over code efficiency. For example, in parts of the notebook, variables are duplicated in order to make the code easier to read and understand. \n",
    "\n",
    "3) This notebook was run on [Amazon Sagemaker](https://aws.amazon.com/sagemaker/) using ml.t2.xlarge. Some of the imports may be slightly different if you are using Google Colab or some other system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start off by loading the basics. This first step is to make sure that that your imports take in the most recent changes to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "% autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, a good portion of python packages are installed with a tool called pip. The following installed the most current version of pip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: pip in /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages (19.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, we are just going to put together a data pipeline. For the example shown, a CPU (ml.t2.xlarge on Amazon Sagemaker) is sufficient enough to do this work.\n",
    "As a result, I install Tensorflow 2.0 without the GPU features. \n",
    "\n",
    "Please note that this notebook only involves the construction of the pipeline. As a result, I don't handle the error associated with the tensorflow-serving-api (which focuses on the deployment of a model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceed (y/n)? yes: standard output: Broken pipe\n",
      "yes: write error\n",
      "\u001b[31mERROR: tensorflow-serving-api 1.14.0 has requirement tensorflow~=1.14.0, but you'll have tensorflow 2.0.0rc1 which is incompatible.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!yes | pip uninstall -q tensorflow\n",
    "!pip install -q tensorflow==2.0.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.python.lib.io.tf_record import TFRecordWriter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0-rc1'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 (Optional): Constructing the Yelp Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is going to construct the tab delimited information that we are going to use in our example. In this example, we are going to look at Yelp in order to\n",
    "understand how to classify comments based on emotion (either 'Positive' or 'Negative'). To make sure that we have our process correct, we can verify our results against the rating\n",
    "that a customer gives to yelp ('the true representation of sentiment').\n",
    "\n",
    "The approach here can then be easily extended to other types of social media (e.g. Twitter, Blogs, etc).\n",
    "\n",
    "The code uses a python package called requests in order to extract 50 stores from Arizona, California, Michigan, Texas, Washington, Florida, and Ohio. Then the code will pull 3 reviews from each of the stores. The net result is to get at least 750 instances of good data (the rough math is that we would get 50*3*7=1050 reviews, but there will be some defective data, and this will reduce the final amount of data collected).\n",
    "\n",
    "\n",
    "Note: In order for this code to work, you will need to go onto the Yelp website, register for a developer account, and get your own api_key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -q requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key='<put your YELP api key here>'\n",
    "headers = {'Authorization': 'Bearer %s' % api_key}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = ['AZ', 'CA', 'MI', 'TX', 'WA', 'FL', 'OH']\n",
    "\n",
    "api_calls = []\n",
    "\n",
    "for state in states:\n",
    "    params={'term': 'AT&T', 'location' : state, 'limit': '50'}\n",
    "    req = requests.get('https://api.yelp.com/v3/businesses/search', \n",
    "                            params=params, headers=headers)\n",
    "    api_calls.append(req)\n",
    "    time.sleep(5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_ids = []\n",
    "\n",
    "for api_call in api_calls:\n",
    "    try:\n",
    "        yelp_json = json.loads(api_call.text)\n",
    "        for shop_json in yelp_json['businesses']:\n",
    "            att_ids.append(shop_json['id'])\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted in the code below, that public APIs don't work well when requests are made without any kind of delay. I put ```time.sleep(1)``` in the code in order to increase the chances that the public api will return back with usable information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 1\n",
    "negative = '1'\n",
    "neutral = '2'\n",
    "positive = '3'\n",
    "\n",
    "sentiment_lookup = {'1': negative, '2': negative, '3': neutral, '4': positive, '5': positive}\n",
    "\n",
    "\n",
    "with open('data/Yelp-ATT-Social-Media.tsv', 'wt') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    tsv_writer.writerow(['idx', 'utterance', 'sentiment'])\n",
    "    \n",
    "    for att_id in att_ids:\n",
    "        url = f'https://api.yelp.com/v3/businesses/{att_id}/reviews'\n",
    "        # Get the 3 reviews per shop\n",
    "        try:\n",
    "            time.sleep(1)\n",
    "            req = requests.get(url,  headers=headers)\n",
    "            yelp_json = json.loads(req.text)\n",
    "\n",
    "            for review in yelp_json['reviews']:\n",
    "                sentiment = sentiment_lookup[str(review['rating'])]\n",
    "                tsv_writer.writerow([str(idx), review['text'], sentiment])\n",
    "                idx = idx + 1\n",
    "        except: \n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even if you have not run the above code, I have already aggreated this information (for \"public use\" and non-commerical, research purposes). This aggregate information is in \n",
    "the data folder in the Yelp-ATT-Social-Media.tsv file. \n",
    "\n",
    "It is important to note that this file really just represents Yelp comments that were pulled with the keyword AT&T. Because of the way that the Yelp API is setup, this means that the\n",
    "information comes from more than just AT&T Stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Yelp-ATT-Social-Media.tsv\", sep=\"\\t\", header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the Yelp code above and in the following Pandas dataframe, I have the following coding for sentiment : '1': Negative, '2': Neutral, '3': Positive. For simplicity and \n",
    "teaching purposes, we will only look at the case, positive and negative responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>utterance</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Ok so I finally amped myself up enough to go into the store about a wifi issue I'm having with my phone. I was ready to go down the list of everything I...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Came into this location to make a cash payment arrived at 2:10pm today , went to try to use the kiosk they have inside the store , warning don't use it it's...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Its. Very bad service And big lier people work there\\nI buy new phone there after when i open the phone i see it's used \\nI back to at&amp;t we tell me this not...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>I've been with AT&amp;T since 2009. I went into the store to purchase another phone and add another phone line to my account. The first five minutes of the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>The most amazing customer service ever!!! Went to upgrade 4 lines. I was assisted by quasim, garrett, and Katie.  These three employees are a dream team! I...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   idx  \\\n",
       "0  1     \n",
       "1  2     \n",
       "2  3     \n",
       "3  4     \n",
       "4  5     \n",
       "\n",
       "                                                                                                                                                         utterance  \\\n",
       "0  Ok so I finally amped myself up enough to go into the store about a wifi issue I'm having with my phone. I was ready to go down the list of everything I...       \n",
       "1  Came into this location to make a cash payment arrived at 2:10pm today , went to try to use the kiosk they have inside the store , warning don't use it it's...   \n",
       "2  Its. Very bad service And big lier people work there\\nI buy new phone there after when i open the phone i see it's used \\nI back to at&t we tell me this not...   \n",
       "3  I've been with AT&T since 2009. I went into the store to purchase another phone and add another phone line to my account. The first five minutes of the...        \n",
       "4  The most amazing customer service ever!!! Went to upgrade 4 lines. I was assisted by quasim, garrett, and Katie.  These three employees are a dream team! I...    \n",
       "\n",
       "   sentiment  \n",
       "0  3          \n",
       "1  1          \n",
       "2  1          \n",
       "3  1          \n",
       "4  3          "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove neutral responses from this teaching example. \n",
    "neutral_responses = df[df['sentiment'] == 2]\n",
    "df = df.drop(neutral_responses.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the main insights that I have seen is that <b> deep learning algorithms do not train well if the training data is skewed and if the deep learning \"loss\" function does not account for this imbalance. </b>\n",
    "\n",
    "For simplicity, we will make sure that there is an equal number of positive and negative responses in our training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that the split between negative and positive is equally balanced in order to get effective training.\n",
    "df_positive = df[df['sentiment'] == 3]\n",
    "df_negative = df[df['sentiment'] == 1]\n",
    "\n",
    "df_train_positive = df_positive.sample(369)\n",
    "df_positive_remaining = df_positive.drop(df_train_positive.index)\n",
    "df_validate_positive = df_positive_remaining.sample(frac=0.5)\n",
    "df_test_positive = df_positive_remaining.drop(df_validate_positive.index)\n",
    "\n",
    "assert len(df_train_positive) + len(df_validate_positive) + len(df_test_positive) == len(df[df['sentiment'] == 3])\n",
    "\n",
    "df_train_negative = df_negative.sample(369)\n",
    "df_negative_remaining = df_negative.drop(df_train_negative.index)\n",
    "df_validate_negative = df_negative_remaining.sample(frac=0.5)\n",
    "df_test_negative = df_negative_remaining.drop(df_validate_negative.index)\n",
    "\n",
    "assert len(df_train_negative) + len(df_validate_negative) + len(df_test_negative) == len(df[df['sentiment'] == 1])\n",
    "\n",
    "assert len(df_train_positive) == len(df_train_negative)\n",
    "\n",
    "train_df = pd.concat([df_train_positive, df_train_negative]).sample(frac=1)\n",
    "validate_df = pd.concat([df_validate_positive, df_validate_negative]).sample(frac=1)\n",
    "test_df = pd.concat([df_test_positive, df_test_negative]).sample(frac=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to this point, the data had been split and examined using a popular python package called Pandas. Now, just use only a numpy representation of the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = train_df.values\n",
    "validate_csv = validate_df.values\n",
    "test_csv = test_df.values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Build the TFRecords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our example is taking data (the CSV values that were extracted from Pandas dataframes), and converting that into a string that is written out to a TFRecord. Although our example fits within memory, it is easy to see how the following for loop could be extended to work with streaming social media data. \n",
    "\n",
    "Beause the TFRecord could be generated from streams (a series of files representing potentially infinite data), we need to eventually tell the model how many steps that we need to process. When setting up \"in memory\" examples in order to set up a pipeline, I have found it useful to capture the size of the train, validation, and test datasets. This is captured in the following generate_json_info function and in the corresponding \"data/yelp_info.json\" file. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_example(features, label):\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'idx': tf.train.Feature(int64_list=tf.train.Int64List(value=[features[0]])),\n",
    "        'sentence': tf.train.Feature(bytes_list=tf.train.BytesList(value=[features[1].encode('utf-8')])),\n",
    "        'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[label]))\n",
    "    }))\n",
    "\n",
    "    return tf_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/yelp_train.tfrecord: --- 0.03516888618469238 seconds ---\n",
      "data/yelp_validate.tfrecord: --- 0.004355669021606445 seconds ---\n",
      "data/yelp_test.tfrecord: --- 0.004141807556152344 seconds ---\n"
     ]
    }
   ],
   "source": [
    "def convert_csv_to_tfrecord(csv, file_name):\n",
    "    start_time = time.time()\n",
    "    writer = TFRecordWriter(file_name)\n",
    "    for index, row in enumerate(csv):\n",
    "        try:\n",
    "            if row is None:\n",
    "                raise Exception('Row Missing')\n",
    "            if row[0] is None or row[1] is None or row[2] is None:\n",
    "                raise Exception('Value Missing')\n",
    "            if row[1].strip() is '':\n",
    "                raise Exception('Utterance is empty')\n",
    "            features, label = row[:-1], row[-1]\n",
    "            example = create_tf_example(features, label)\n",
    "            writer.write(example.SerializeToString())\n",
    "        except Exception as inst:\n",
    "            print(type(inst))\n",
    "            print(inst.args)\n",
    "            print(inst)\n",
    "    writer.close()\n",
    "    print(f\"{file_name}: --- {(time.time() - start_time)} seconds ---\")\n",
    "\n",
    "def generate_json_info(local_file_name):\n",
    "    info = {\"train_length\": len(train_df), \"validation_length\": len(validate_df),\n",
    "            \"test_length\": len(test_df)}\n",
    "\n",
    "    with open(local_file_name, 'w') as outfile:\n",
    "        json.dump(info, outfile)\n",
    "    \n",
    "    \n",
    "convert_csv_to_tfrecord(train_csv, \"data/yelp_train.tfrecord\")\n",
    "convert_csv_to_tfrecord(validate_csv, \"data/yelp_validate.tfrecord\")\n",
    "convert_csv_to_tfrecord(test_csv, \"data/yelp_test.tfrecord\")\n",
    "\n",
    "generate_json_info(\"data/yelp_info.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Confirm that TFRecord has encoded correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you generate the TFRecords and the corresponding size of the records, it is good practice to make sure that everything encoded correctly. We can confirm this by setting up code to pull from the TFRecords, setting the Tensors that we want to extract, and setting up a python iterator to briefly inspect the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_ds = tf.data.TFRecordDataset(\"data/yelp_train.tfrecord\")        # The dataset for train information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you know beforehand the number of items in a feature, you should use FixedLenFeature. If you do not, use a VarLenFeature. In our case, we know that we are taking in 1 index, 1 text utterance (the 'sentence'), and 1 label per example. As a result, each of these are FixedLenFeatures . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_spec = {\n",
    "    'idx': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'sentence': tf.io.FixedLenFeature([], tf.string),\n",
    "    'label': tf.io.FixedLenFeature([], tf.int64)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_example(example_proto):\n",
    "  # Parse the input tf.Example proto using the dictionary above.\n",
    "  return tf.io.parse_single_example(example_proto, feature_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_parse_ds = tr_ds.map(parse_example)\n",
    "val_parse_ds = val_ds.map(parse_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_iterator = iter(tr_parse_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'idx': <tf.Tensor: id=56, shape=(), dtype=int64, numpy=367>,\n",
       " 'label': <tf.Tensor: id=57, shape=(), dtype=int64, numpy=3>,\n",
       " 'sentence': <tf.Tensor: id=58, shape=(), dtype=string, numpy=b'This is a Metro PCS store that excels in customer service! The manager, all the way down to the staff are great at assisting customers. Almost everyone in...'>}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_iterator.get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Congratulations! At this point you now have your data encoded as a TFRecord. You are now in a position to feed this information to a data pipeline (tf.data.Dataset) which makes it incredibly easy to then push that data to two types of Tensorflow modeling frameworks (the Keras model or the TensorFlow Estimator). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the point where you are ready, take a look at the next tutorial in this respository [(\"2-Sentiment-Classification-with-BERT.ipynb\")](https://github.com/ralphbrooks/tensorflow-tutorials/blob/master/2-Sentiment-Classification-with-BERT.ipynb) .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, for the first time in 10 years, I am back on the job market looking for consulting opportunities or full time employment.  If you think I can be of help to you, feel free to reach out. I am on twitter at [@ralphbrooks](https://twitter.com/ralphbrooks) ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
